{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlling the MLSVD truncation \n",
    "\n",
    "One of the most important things one need to get right is the truncation of the MLSVD. It doesn't matter how optimized are the CPD routines, it will take a long time to finish the computations if the tensor is big and no compression is made. Many other CPD solvers out there skip the compression stage, and this is one of the reasons (but not the only one) why Tensor Fox is faster. In this lesson we will see some tricks to obtain a decent preprocessed tensor in which we will compute its CPD. Let's work again with the swimmer tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import TensorFox as tfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensor T. \n",
    "T = np.load('swimmer.npy')\n",
    "dims = T.shape\n",
    "L = len(T.shape)\n",
    "Tsize = np.linalg.norm(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will consider the rank as $R = 50$. As mentioned in lesson 3, the reduced MLSVD of $T$ is given by $T = (U_1, U_2, U_3) \\cdot S$, where $S \\in \\mathbb{R}^{R_1 \\times R_2 \\times R_3}$, $U_1 \\in \\mathbb{R}^{256 \\times R_1}$, $U_2 \\in \\mathbb{R}^{32 \\times R_2}$, $U_3 \\in \\mathbb{R}^{32 \\times R_3}$. The tuple $(R_1, R_2, R_3)$ is the multilinear rank of $T$. When the **cpd** function is called, its first action is to run a MLSVD routine and estimate the values $R_1, R_2, R_3$. Since these values are obtained by approximation, the resulting core tensor $S$ is actually a truncation. Below we run the **cpd** with $T$ to remember this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-> With display > 2 Tensor Fox will compute the error of the MLSVD. For large tensors this routine is very clostly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    Compressing unfolding mode 1\n",
      "    Compressing unfolding mode 2\n",
      "    Compressing unfolding mode 3\n",
      "    Compression detected\n",
      "    Compressing from (256, 32, 32) to (13, 23, 15)\n",
      "    Compression relative error = 1.459281e-10\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "    Initial guess relative error = 1.341551e+00\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "    Iteration | Rel error |  Step size  | Improvement | norm(grad) | Predicted error | # Inner iterations\n",
      "        1     | 1.25e+00  |  4.73e-02   |  1.25e+00   |  2.17e+03  |    5.07e-03     |        2        \n",
      "        2     | 1.21e+00  |  1.97e-02   |  3.45e-02   |  1.66e+03  |    1.01e-04     |        3        \n",
      "        3     | 1.12e+00  |  5.68e-02   |  8.98e-02   |  1.51e+03  |    1.61e-04     |        3        \n",
      "        4     | 8.52e-01  |  1.65e-01   |  2.69e-01   |  1.82e+03  |    2.91e-04     |        3        \n",
      "        5     | 3.34e-01  |  3.03e-01   |  5.19e-01   |  3.42e+03  |    3.68e-03     |        4        \n",
      "        6     | 1.96e-01  |  1.15e-01   |  1.38e-01   |  2.85e+03  |    8.58e-05     |        6        \n",
      "        7     | 1.54e-01  |  1.32e-01   |  4.21e-02   |  2.36e+02  |    4.31e-03     |        6        \n",
      "        8     | 1.20e-01  |  1.74e-01   |  3.37e-02   |  3.09e+02  |    1.46e-03     |        5        \n",
      "        9     | 1.00e-01  |  1.77e-01   |  1.96e-02   |  1.57e+02  |    5.24e-04     |        8        \n",
      "       10     | 9.20e-02  |  9.92e-02   |  8.26e-03   |  5.97e+01  |    1.73e-03     |        8        \n",
      "       11     | 8.38e-02  |  1.05e-01   |  8.24e-03   |  1.60e+02  |    7.32e-04     |        4        \n",
      "       12     | 7.73e-02  |  9.05e-02   |  6.48e-03   |  1.03e+02  |    8.96e-04     |        6        \n",
      "       13     | 6.99e-02  |  1.45e-01   |  7.38e-03   |  1.11e+02  |    1.65e-04     |        7        \n",
      "       14     | 6.68e-02  |  7.25e-02   |  3.11e-03   |  3.56e+01  |    1.24e-04     |        12       \n",
      "       15     | 6.50e-02  |  4.70e-02   |  1.77e-03   |  2.76e+01  |    1.30e-04     |        8        \n",
      "       16     | 6.09e-02  |  1.30e-01   |  4.11e-03   |  3.51e+01  |    1.04e-04     |        13       \n",
      "       17     | 5.90e-02  |  5.11e-02   |  1.92e-03   |  2.35e+01  |    2.89e-05     |        13       \n",
      "       18     | 5.86e-02  |  9.84e-03   |  4.71e-04   |  8.63e+00  |    2.60e-05     |        7        \n",
      "       19     | 5.82e-02  |  9.08e-03   |  3.48e-04   |  7.43e+00  |    3.38e-05     |        14       \n",
      "       20     | 5.43e-02  |  1.26e-01   |  3.86e-03   |  1.80e+01  |    3.08e-05     |        12       \n",
      "       21     | 5.28e-02  |  3.51e-02   |  1.53e-03   |  2.43e+01  |    6.16e-05     |        13       \n",
      "       22     | 4.87e-02  |  1.35e-01   |  4.10e-03   |  3.28e+01  |    3.70e-05     |        13       \n",
      "       23     | 4.73e-02  |  2.99e-02   |  1.45e-03   |  2.31e+01  |    2.31e-05     |        10       \n",
      "       24     | 4.59e-02  |  4.86e-02   |  1.34e-03   |  1.52e+01  |    8.49e-05     |        13       \n",
      "       25     | 4.27e-02  |  1.25e-01   |  3.21e-03   |  3.74e+01  |    1.93e-05     |        15       \n",
      "       26     | 4.16e-02  |  2.20e-02   |  1.11e-03   |  1.30e+01  |    3.48e-05     |        9        \n",
      "       27     | 3.97e-02  |  7.13e-02   |  1.90e-03   |  1.42e+01  |    2.84e-05     |        14       \n",
      "       28     | 3.87e-02  |  3.62e-02   |  9.78e-04   |  1.93e+01  |    1.91e-05     |        11       \n",
      "       29     | 3.81e-02  |  2.79e-02   |  5.96e-04   |  1.46e+01  |    1.39e-05     |        11       \n",
      "       30     | 3.72e-02  |  4.50e-02   |  9.41e-04   |  1.09e+01  |    1.72e-05     |        22       \n",
      "       31     | 3.55e-02  |  7.85e-02   |  1.72e-03   |  5.32e+00  |    1.35e-05     |        23       \n",
      "       32     | 3.47e-02  |  1.91e-02   |  7.33e-04   |  1.33e+01  |    3.08e-05     |        8        \n",
      "       33     | 3.28e-02  |  1.48e-01   |  1.95e-03   |  1.95e+01  |    1.04e-05     |        23       \n",
      "       34     | 3.09e-02  |  2.34e-02   |  1.91e-03   |  1.63e+01  |    1.05e-05     |        13       \n",
      "       35     | 2.94e-02  |  1.00e-01   |  1.50e-03   |  1.21e+01  |    7.37e-06     |        24       \n",
      "       36     | 2.81e-02  |  3.01e-02   |  1.26e-03   |  8.40e+00  |    5.19e-06     |        20       \n",
      "       37     | 2.77e-02  |  1.92e-02   |  3.96e-04   |  5.73e+00  |    5.58e-06     |        15       \n",
      "       38     | 2.72e-02  |  2.22e-02   |  4.68e-04   |  5.64e+00  |    1.33e-05     |        16       \n",
      "       39     | 2.66e-02  |  3.17e-02   |  6.11e-04   |  1.75e+01  |    1.69e-05     |        11       \n",
      "       40     | 2.61e-02  |  2.14e-02   |  5.76e-04   |  1.85e+01  |    3.38e-06     |        12       \n",
      "       41     | 2.51e-02  |  4.61e-02   |  9.25e-04   |  4.84e+00  |    1.01e-05     |        24       \n",
      "       42     | 2.43e-02  |  3.15e-02   |  8.26e-04   |  4.66e+00  |    8.18e-06     |        18       \n",
      "       43     | 2.30e-02  |  5.97e-02   |  1.27e-03   |  9.29e+00  |    1.06e-05     |        29       \n",
      "       44     | 2.20e-02  |  3.59e-02   |  9.87e-04   |  1.26e+01  |    3.67e-06     |        29       \n",
      "       45     | 2.15e-02  |  2.46e-02   |  5.53e-04   |  4.76e+00  |    4.18e-06     |        30       \n",
      "       46     | 2.11e-02  |  2.02e-02   |  4.02e-04   |  6.47e+00  |    3.88e-06     |        18       \n",
      "       47     | 2.07e-02  |  1.95e-02   |  3.65e-04   |  7.51e+00  |    5.20e-06     |        17       \n",
      "       48     | 2.05e-02  |  9.61e-03   |  2.54e-04   |  9.02e+00  |    3.64e-06     |        7        \n",
      "       49     | 1.91e-02  |  6.92e-02   |  1.36e-03   |  8.90e+00  |    6.05e-06     |        16       \n",
      "       50     | 1.86e-02  |  5.80e-03   |  5.00e-04   |  4.10e+00  |    4.50e-06     |        6        \n",
      "       51     | 1.84e-02  |  7.99e-03   |  2.43e-04   |  6.43e+00  |    8.55e-07     |        6        \n",
      "       52     | 1.77e-02  |  2.80e-02   |  6.34e-04   |  1.58e+00  |    8.27e-06     |        25       \n",
      "       53     | 1.73e-02  |  1.19e-02   |  4.19e-04   |  1.18e+01  |    4.41e-06     |        11       \n",
      "       54     | 1.68e-02  |  2.08e-02   |  4.89e-04   |  9.49e+00  |    7.07e-06     |        11       \n",
      "       55     | 1.65e-02  |  1.00e-02   |  3.65e-04   |  1.13e+01  |    1.04e-06     |        11       \n",
      "       56     | 1.55e-02  |  4.99e-02   |  9.86e-04   |  1.84e+00  |    2.57e-06     |        30       \n",
      "       57     | 1.51e-02  |  6.94e-03   |  3.45e-04   |  7.30e+00  |    2.01e-06     |        12       \n",
      "       58     | 1.45e-02  |  4.08e-02   |  5.92e-04   |  6.02e+00  |    3.44e-06     |        21       \n",
      "       59     | 1.41e-02  |  1.81e-02   |  4.30e-04   |  8.64e+00  |    6.67e-07     |        32       \n",
      "       60     | 1.37e-02  |  4.22e-02   |  3.99e-04   |  2.42e+00  |    1.28e-06     |        31       \n",
      "       61     | 1.34e-02  |  1.10e-02   |  3.53e-04   |  2.98e+00  |    3.68e-07     |        16       \n",
      "       62     | 1.30e-02  |  2.75e-02   |  3.11e-04   |  2.02e+00  |    1.20e-06     |        42       \n",
      "       63     | 1.29e-02  |  2.54e-03   |  1.12e-04   |  4.12e+00  |    1.02e-06     |        9        \n",
      "       64     | 1.24e-02  |  3.87e-02   |  4.85e-04   |  4.45e+00  |    1.07e-06     |        29       \n",
      "       65     | 1.60e-02  |  7.18e-02   |  3.57e-03   |  1.06e+01  |    1.74e-06     |        38       \n",
      "       66     | 8.61e-03  |  4.83e-02   |  7.41e-03   |  1.00e+02  |    1.15e-06     |        43       \n",
      "       67     | 6.16e-03  |  4.42e-02   |  2.45e-03   |  4.46e+01  |    8.22e-07     |        39       \n",
      "       68     | 4.83e-03  |  6.86e-03   |  1.33e-03   |  2.30e+01  |    2.56e-07     |        15       \n",
      "       69     | 4.49e-03  |  1.58e-02   |  3.43e-04   |  2.31e+00  |    1.66e-07     |        37       \n",
      "       70     | 4.07e-03  |  2.67e-02   |  4.13e-04   |  1.57e+00  |    3.49e-07     |        39       \n",
      "       71     | 3.65e-03  |  1.20e-02   |  4.21e-04   |  4.93e+00  |    1.05e-07     |        47       \n",
      "       72     | 3.50e-03  |  8.26e-03   |  1.51e-04   |  1.71e+00  |    8.25e-08     |        38       \n",
      "       73     | 3.37e-03  |  8.47e-03   |  1.31e-04   |  1.20e+00  |    1.16e-07     |        40       \n",
      "       74     | 3.29e-03  |  3.49e-03   |  8.18e-05   |  1.77e+00  |    3.96e-08     |        21       \n",
      "       75     | 3.27e-03  |  9.82e-04   |  2.25e-05   |  5.06e-01  |    1.58e-08     |        13       \n",
      "       76     | 4.39e-03  |  6.75e-02   |  1.12e-03   |  5.94e-01  |    1.29e-07     |        40       \n",
      "       77     | 2.20e-03  |  8.18e-03   |  2.19e-03   |  8.07e+00  |    2.41e-08     |        32       \n",
      "       78     | 2.14e-03  |  3.01e-03   |  6.49e-05   |  8.46e-01  |    1.63e-08     |        33       \n",
      "       79     | 2.06e-03  |  7.73e-03   |  8.10e-05   |  3.37e-01  |    4.10e-08     |        42       \n",
      "       80     | 2.00e-03  |  1.88e-03   |  6.05e-05   |  1.42e+00  |    8.14e-09     |        27       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       81     | 1.95e-03  |  6.03e-03   |  4.47e-05   |  5.18e-01  |    5.21e-08     |        27       \n",
      "       82     | 1.89e-03  |  2.35e-03   |  5.63e-05   |  7.20e-01  |    6.81e-09     |        46       \n",
      "       83     | 1.88e-03  |  1.21e-03   |  1.56e-05   |  3.13e-01  |    3.39e-09     |        24       \n",
      "       84     | 1.83e-03  |  6.97e-03   |  4.59e-05   |  1.74e-01  |    4.34e-08     |        39       \n",
      "       85     | 1.78e-03  |  1.86e-02   |  5.69e-05   |  4.49e-01  |    7.49e-08     |        55       \n",
      "       86     | 1.56e-03  |  2.35e-03   |  2.19e-04   |  1.22e+00  |    2.78e-09     |        30       \n",
      "       87     | 1.54e-03  |  2.57e-03   |  2.19e-05   |  1.21e-01  |    5.14e-09     |        39       \n",
      "       88     | 1.51e-03  |  2.13e-03   |  2.26e-05   |  1.62e-01  |    4.88e-09     |        39       \n",
      "       89     | 1.50e-03  |  1.46e-03   |  1.57e-05   |  2.07e-01  |    3.23e-09     |        39       \n",
      "       90     | 1.49e-03  |  7.99e-04   |  7.73e-06   |  3.47e-01  |    2.95e-09     |        16       \n",
      "       91     | 1.48e-03  |  1.43e-02   |  9.49e-06   |  2.50e-01  |    4.44e-08     |        44       \n",
      "       92     | 1.33e-03  |  1.97e-03   |  1.48e-04   |  7.60e-01  |    5.55e-10     |        30       \n",
      "       93     | 1.32e-03  |  1.07e-03   |  7.20e-06   |  1.13e-01  |    2.56e-09     |        22       \n",
      "       94     | 1.32e-03  |  4.83e-04   |  5.77e-06   |  1.65e-01  |    5.44e-10     |        25       \n",
      "       95     | 1.32e-03  |  4.00e-04   |  2.64e-06   |  5.57e-02  |    6.91e-10     |        13       \n",
      "       96     | 1.31e-03  |  1.65e-04   |  1.54e-06   |  6.67e-02  |    5.04e-10     |        10       \n",
      "       97     | 1.80e-03  |  3.31e-02   |  4.84e-04   |  4.70e-02  |    5.79e-08     |        22       \n",
      "       98     | 1.04e-03  |  2.07e-03   |  7.55e-04   |  2.58e+00  |    2.77e-09     |        23       \n",
      "       99     | 1.03e-03  |  6.38e-04   |  9.67e-06   |  3.56e-01  |    1.34e-09     |        23       \n",
      "       100    | 1.03e-03  |  3.66e-04   |  3.57e-06   |  1.48e-01  |    5.48e-10     |        23       \n",
      "       101    | 1.02e-03  |  1.30e-03   |  6.50e-06   |  3.72e-02  |    1.56e-09     |        27       \n",
      "       102    | 1.05e-03  |  1.08e-02   |  2.49e-05   |  7.68e-02  |    6.27e-09     |        51       \n",
      "       103    | 9.46e-04  |  1.54e-03   |  1.01e-04   |  5.46e-01  |    7.84e-10     |        29       \n",
      "       104    | 9.40e-04  |  8.27e-04   |  6.63e-06   |  8.34e-02  |    6.28e-10     |        25       \n",
      "       105    | 9.44e-04  |  7.46e-03   |  4.64e-06   |  4.46e-02  |    2.18e-09     |        47       \n",
      "       106    | 8.94e-04  |  1.17e-03   |  5.06e-05   |  3.82e-01  |    5.08e-10     |        27       \n",
      "       107    | 8.91e-04  |  2.41e-04   |  2.81e-06   |  6.53e-02  |    3.02e-10     |        10       \n",
      "       108    | 8.82e-04  |  3.67e-03   |  9.13e-06   |  4.26e-02  |    3.93e-10     |        60       \n",
      "       109    | 8.71e-04  |  2.14e-04   |  1.02e-05   |  1.89e-01  |    2.81e-10     |        11       \n",
      "       110    | 8.63e-04  |  3.42e-03   |  8.93e-06   |  4.50e-02  |    5.36e-10     |        57       \n",
      "       111    | 8.45e-04  |  2.29e-03   |  1.71e-05   |  1.68e-01  |    3.53e-10     |        48       \n",
      "       112    | 8.40e-04  |  4.90e-03   |  5.61e-06   |  1.30e-01  |    1.04e-09     |        64       \n",
      "       113    | 8.15e-04  |  6.21e-04   |  2.47e-05   |  2.43e-01  |    5.32e-10     |        19       \n",
      "       114    | 8.14e-04  |  1.31e-04   |  1.58e-06   |  4.97e-02  |    2.57e-10     |        12       \n",
      "       115    | 8.07e-04  |  1.44e-03   |  6.33e-06   |  7.07e-02  |    3.93e-10     |        41       \n",
      "       116    | 8.05e-04  |  2.18e-04   |  2.52e-06   |  9.70e-02  |    1.66e-10     |        20       \n",
      "       117    | 8.01e-04  |  3.07e-03   |  3.70e-06   |  3.18e-02  |    6.23e-10     |        41       \n",
      "       118    | 7.84e-04  |  1.64e-03   |  1.66e-05   |  1.50e-01  |    2.57e-10     |        53       \n",
      "       119    | 7.78e-04  |  1.39e-03   |  6.05e-06   |  1.08e-01  |    3.04e-10     |        36       \n",
      "       120    | 7.72e-04  |  1.50e-03   |  6.58e-06   |  8.03e-02  |    3.05e-10     |        56       \n",
      "       121    | 7.67e-04  |  7.62e-04   |  5.29e-06   |  8.85e-02  |    2.01e-10     |        31       \n",
      "       122    | 7.63e-04  |  9.43e-04   |  3.37e-06   |  5.82e-02  |    5.72e-10     |        37       \n",
      "       123    | 7.60e-04  |  4.30e-04   |  3.46e-06   |  7.63e-02  |    8.98e-11     |        43       \n",
      "       124    | 1.07e-03  |  2.03e-02   |  3.15e-04   |  2.56e-02  |    4.92e-09     |        57       \n",
      "       125    | 6.56e-04  |  1.94e-03   |  4.18e-04   |  1.29e+00  |    8.43e-11     |        57       \n",
      "       126    | 6.53e-04  |  1.24e-04   |  3.13e-06   |  9.92e-02  |    5.64e-11     |        20       \n",
      "       127    | 6.54e-04  |  3.47e-03   |  6.96e-07   |  3.81e-02  |    1.21e-09     |        64       \n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 127\n",
      "    Relative error = 0.0006529150573275845\n",
      "    Accuracy =  99.93471 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the CPD of T, assuming T has rank 50. \n",
    "class options:\n",
    "    display = 3\n",
    "    \n",
    "R = 50\n",
    "factors, output = tfx.cpd(T, R, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we know for sure is that $R_1 \\leq \\min\\{ R, m \\}$, $R_2 \\leq \\min\\{ R, n \\}$, $R_3 \\leq \\min\\{ R, p \\}$, where $m, n, p$ are the dimensions of $T$. This is valid in general for order-$L$ tensors, i.e., $R_\\ell \\leq \\min\\{ R, I_\\ell \\}$ for all $\\ell = 1 \\ldots L$. For example, in the case of the swimmer tensor this tuple is $(50, 32, 32)$, but we just saw that the tuple $(13, 23, 15)$ is already very good (with error of order $10^{-10}$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncation testing \n",
    "\n",
    "In more difficult situations one may be interested in testing a list of several possible truncations and choose one of them. This can be accomplished with the function **test_truncation**. Below we will show an example with the swimmer tensor. \n",
    "\n",
    "**PS:** The ordering of the dimensions are different here. The **cpd** function sorts the dimension of $T$ in descending order, so the original tensor with shape $32 \\times 32 \\times 256$ is rearranged to have shape $256 \\times 32 \\times 32$. The truncation testing doesn't change the dimension ordering, so the optimal truncation $(13, 23, 15)$ mentioned before is for the sorted $T$. In this context the optimal truncation now is $(23, 15, 13)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended truncation = (23, 15, 13)     Relative error = 1.0097645776857521e-15 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List a few truncations (a, b, c) wherea a <= 32, b <= 32 and c <= 50. \n",
    "trunc_list = [(2, 3, 5), (5, 5, 10), (10, 10, 10), (22, 14, 12), (23, 14, 13), (23, 15, 13), (30, 30, 30), (32, 32, 50)]\n",
    "\n",
    "# Start the tests.\n",
    "trunc_error = tfx.test_truncation(T, trunc_list, display=False)\n",
    "\n",
    "# Now we plot the evolution of the energy and the corresponding errors.\n",
    "df = pd.DataFrame(index= [str(x) for x in trunc_list], data=trunc_error, columns=['Error'])\n",
    "\n",
    "# Print the first truncation with error than 1e-13.\n",
    "for idx in df.index:\n",
    "    if df.loc[idx, 'Error'] < 1e-13:\n",
    "        print('Recommended truncation =', idx, '    Relative error =', df.loc[idx, 'Error'], '\\n')\n",
    "        break\n",
    "\n",
    "df.plot(figsize=[16, 4], grid=True, legend=[], marker='s')\n",
    "plt.ylabel('Relative Error')\n",
    "plt.xlabel('Truncation')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we need to do this if Tensor Fox is better?\n",
    "\n",
    "Actually, the original purpose function **test_truncation** wasn't to find the optimal truncation, but to understand how the error changes as the truncation changes. In general the error behaves as the example above, i.e., it doesn't change gradually but abruptly. \n",
    "\n",
    "Also it is important to remark that Tensor Fox works with approximations (like any solver), therefore the truncation given by Tensor Fox may not be always the optimal one. In that case it would be interesting for the user to use this truncation as starting point to test closer truncations. Sometimes they can find a smaller truncation with the same relative error. That wasn't the case for this particular example because Tensor Fox did found the optimal truncation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the compressed tensor instead of the original one\n",
    "\n",
    "Just a quick recap: \n",
    " - We have our original raw tensor $T$, which is big and probably can be compressed.\n",
    " - $T = (U_1, \\ldots, U_L) \\cdot S$, where $S$ is the *core* tensor of the MLSVD of $T$. It is the compressed version of $T$.\n",
    " - All the talk above about truncation was about $S$, not $T$, you probably understand this at this point.\n",
    " - The dimensions of $S$ are $(R_1, \\ldots, R_L)$ and they satisfy $R_\\ell \\leq \\min\\{I_\\ell, R\\}$, where $I_\\ell$ is the $\\ell$-th dimension of $T$ and $R$ is the rank of $T$.\n",
    " - The CPD will be computed for $S$, because its cheaper to compute this CPD and once we have this CPD, the CPD of $T$ is easily obtained. \n",
    " - When we call the function **cpd** with Tensor Fox, it computes the compression $S$ and then the CPD of $S$. \n",
    " \n",
    "Assume that the compression computed is precise enough. Still we can have bad luck when computing the CPD, that is, the program may converge to a local minimum. The solution won't be close enough and we will have to run the **cpd** all over again. Maybe the second time it is better (and probably will be if the problem is not ill-posed). Regardless, we are computing the compression twice, once per **cpd** call. If the tensor is very large, we don't want to repeat this step several times.\n",
    "\n",
    "**The solution is**: We compute compression by ourselves and store it, then we can compute its CPD several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start creating the class of options.\n",
    "class options:    \n",
    "    # Define the error tolerance but do not use a value <= 0 to compute the compression directly like we are doing now.\n",
    "    tol_mlsvd = 1e-16    \n",
    "    \n",
    "# Call the MLSVD function. Only S and U matters for us.\n",
    "S, U, T1, sigmas = tfx.mlsvd(T, Tsize, R, options)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    No compression and no truncation requested by user\n",
      "    Working with dimensions (23, 15, 13)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 136\n",
      "    Relative error = 0.0009060851789082211\n",
      "    Accuracy =  99.90939 %\n"
     ]
    }
   ],
   "source": [
    "# Now we compute the CPD of S.\n",
    "options.tol_mlsvd = -1    # this options makes the program to skip the compression step, which is unnecessary now\n",
    "options.display = 1 \n",
    "factors, output = tfx.cpd(S, R, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have that T = (U1, U2, U3)*S = (U1, U2, U3)*(W1, W2, W3)*I = (U1*W1, U2*W2, U3*W3)*I, where W1, W2, W3 are the\n",
    "# factor matrices of S. Therefore U1*W1, U2*W2, U3*W3 are the factor matrices of T, se we redefine them.\n",
    "factors = [np.dot(u, w) for u, w in zip(U, factors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rel error = 0.0009060851816266081\n"
     ]
    }
   ],
   "source": [
    "# This step you can skip, but we can check the solution by computing the difference between T and its approximation.\n",
    "T_approx = tfx.cpd2tens(factors)\n",
    "print('Rel error =', np.linalg.norm(T - T_approx)/np.linalg.norm(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the compression error is close to $10^{-10}$, we can see the the major part of the error comes from the CPD of $S$. That is why it is interesting to save the compression to work with. If there is a way to improve the CPD it will be through the CPD of $S$, and since this tensor is much smaller than the original one, we can make several experimentations within a reasonable time. To illustrate let's try to improve the previous result by testing some combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cg_factor = 1 cg_tol = 1e-16 cg_maxiter = 10 rel_error = 0.000885356184740611\n",
      "cg_factor = 1 cg_tol = 1e-16 cg_maxiter = 50 rel_error = 0.0009057293015406809\n",
      "cg_factor = 1 cg_tol = 1e-16 cg_maxiter = 100 rel_error = 0.0003955359646412877\n",
      "cg_factor = 1 cg_tol = 1e-32 cg_maxiter = 10 rel_error = 0.002590809595424418\n",
      "cg_factor = 1 cg_tol = 1e-32 cg_maxiter = 50 rel_error = 0.0005355422495779255\n",
      "cg_factor = 1 cg_tol = 1e-32 cg_maxiter = 100 rel_error = 0.0014526032146188585\n",
      "cg_factor = 2 cg_tol = 1e-16 cg_maxiter = 10 rel_error = 0.00029568937368274633\n",
      "cg_factor = 2 cg_tol = 1e-16 cg_maxiter = 50 rel_error = 0.00032737122943516105\n",
      "cg_factor = 2 cg_tol = 1e-16 cg_maxiter = 100 rel_error = 0.0018736910229589859\n",
      "cg_factor = 2 cg_tol = 1e-32 cg_maxiter = 10 rel_error = 0.0009657722050728615\n",
      "cg_factor = 2 cg_tol = 1e-32 cg_maxiter = 50 rel_error = 0.0004036259576516109\n",
      "cg_factor = 2 cg_tol = 1e-32 cg_maxiter = 100 rel_error = 0.0009114227995968617\n",
      "cg_factor = 5 cg_tol = 1e-16 cg_maxiter = 10 rel_error = 0.00031336409213726814\n",
      "cg_factor = 5 cg_tol = 1e-16 cg_maxiter = 50 rel_error = 0.004478897828711603\n",
      "cg_factor = 5 cg_tol = 1e-16 cg_maxiter = 100 rel_error = 0.0004842032892729577\n",
      "cg_factor = 5 cg_tol = 1e-32 cg_maxiter = 10 rel_error = 0.0006447192020486519\n",
      "cg_factor = 5 cg_tol = 1e-32 cg_maxiter = 50 rel_error = 0.00037433163011747436\n",
      "cg_factor = 5 cg_tol = 1e-32 cg_maxiter = 100 rel_error = 0.0008941732872243573\n",
      "cg_factor = 10 cg_tol = 1e-16 cg_maxiter = 10 rel_error = 0.01139437321195756\n",
      "cg_factor = 10 cg_tol = 1e-16 cg_maxiter = 50 rel_error = 0.0004995678876912198\n",
      "cg_factor = 10 cg_tol = 1e-16 cg_maxiter = 100 rel_error = 5.0792302491170266e-05\n",
      "cg_factor = 10 cg_tol = 1e-32 cg_maxiter = 10 rel_error = 0.00026061116975572847\n",
      "cg_factor = 10 cg_tol = 1e-32 cg_maxiter = 50 rel_error = 0.0021201846643137214\n",
      "cg_factor = 10 cg_tol = 1e-32 cg_maxiter = 100 rel_error = 0.00021081432093359158\n",
      "Best model: cg_factor = 10 cg_tol = 1e-16 cg_maxiter = 100\n",
      "Final best rel_error = 5.0792302491170266e-05\n"
     ]
    }
   ],
   "source": [
    "best_error = np.inf\n",
    "\n",
    "for cg_factor in [1, 2, 5, 10]:\n",
    "    for cg_tol in [1e-16, 1e-32]:\n",
    "        for cg_maxiter in [10, 50, 100]:\n",
    "            options.tol_mlsvd = -1 \n",
    "            options.display = 0\n",
    "            options.cg_factor = cg_factor\n",
    "            options.cg_tol = cg_tol\n",
    "            options.cg_maxiter = cg_maxiter\n",
    "            factors, output = tfx.cpd(S, R, options)\n",
    "            print('cg_factor =', cg_factor, 'cg_tol =', cg_tol, 'cg_maxiter =', cg_maxiter, 'rel_error =', output.rel_error)\n",
    "            \n",
    "            if output.rel_error < best_error:\n",
    "                best_error = output.rel_error\n",
    "                best_factors = [np.dot(u, w) for u, w in zip(U, factors)]\n",
    "                best_output = copy(output)\n",
    "    \n",
    "output = best_output\n",
    "factors = best_factors\n",
    "print('Best model: cg_factor =', output.options.cg_factor, 'cg_tol =', output.options.cg_tol, 'cg_maxiter =', output.options.cg_maxiter)\n",
    "print('Final best rel_error =', output.rel_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways of doing this and of course there are many more possibilities. I think you got the idea at this point, this is what really matter. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
