{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse tensors\n",
    "\n",
    "A new functionality of Tensor Fox is the support for sparse tensors. Let $T \\in \\mathbb{R}^{I_1 \\times \\ldots \\times I_L}$ be a sparse tensor with *nnz* nonzero entries. This tensor is represented as a triple *[data, idxs, dims]*, where *data* is an array of size $nnz$ such that *data[i]* is the $i$-th nonzero entry of $T$, with corresponding index *idxs[i]*. It is necessary to pass *idxs* also as an array of shape $nnz \\times L$. Finally, we have that *dims* $= [I_1, \\ldots, I_L]$ are the dimwnsions of the tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy\n",
    "import TensorFox as tfx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small example\n",
    "\n",
    "Below we start with a fourth order sparse tensor $10 \\times 10 \\times 10 \\times 10$ with rank $R = 5$ such that each factor matrix has 5 nonzero entries. We can generate sparse factor matrices with the function *gen_rand_sparse_tensor* with inputs **dims**, **R**, **nnz** as shown below. We remark that the sparsity of the tensor is not necessarily equal to the sparsity of its factor matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor matrix sparsity level = 80.0 %\n",
      "Tensor sparsity level = 91.0 %\n"
     ]
    }
   ],
   "source": [
    "# Dimensions of the tensor.\n",
    "dims = [10, 10, 10, 10]\n",
    "\n",
    "# Rank of the tensor.\n",
    "R = 5\n",
    "\n",
    "# Number of nonzero entries of each factor matrix. \n",
    "nnz = 10\n",
    "\n",
    "# Generate sparse tensor.\n",
    "factor_dims_prod = tfx.multiply_dims([dims[0], R])\n",
    "print('Factor matrix sparsity level =', round(100 * (factor_dims_prod - nnz)/factor_dims_prod, 2), '%')\n",
    "data, idxs, dims, factors = tfx.gen_rand_sparse_tensor(dims, R, nnz)\n",
    "tensor_dims_prod = tfx.multiply_dims(dims)\n",
    "print('Tensor sparsity level =', round(100 * (tensor_dims_prod - len(data))/tensor_dims_prod, 2), '%')\n",
    "\n",
    "# Define sparse tensor.\n",
    "T = [data, idxs, dims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remark that it is not possible to use $\\verb|tol| \\_ \\verb|mlsvd| = -1$ for sparse tensors. In the case the user sets $\\verb|display| = 3$ or $4$, the program computes only the error associated with the nonzero entries of $T$, otherwise we would face memory issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse tensor detected\n",
      "    nnz = 900\n",
      "    Sparsity level = 91.0 %\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    Compressing unfolding mode 1\n",
      "    Compressing unfolding mode 2\n",
      "    Compressing unfolding mode 3\n",
      "    Compressing unfolding mode 4\n",
      "    Computing sparse multilinear multiplication\n",
      "    Compression detected\n",
      "    Compressing from (10, 10, 10, 10) to (5, 5, 5, 5)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 148\n",
      "    Relative error = 4.646602820780112e-15\n",
      "    Accuracy =  100.0 %\n"
     ]
    }
   ],
   "source": [
    "class options:\n",
    "    display = 1\n",
    "    \n",
    "factors, output = tfx.cpd(T, R, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative error above only take in account the nonzero entries of $T$ (the same is true for the compression error when displayed). The factor matrices of the decomposition may introduce small errors when approximating the zeros, and these small errors summed together does increase the actual error of the CPD. Thus the relative error above is a lower bound to the actual error, but usually close enough.\n",
    "\n",
    "We can convert the factor matrices to tensor sparse format just like the original input. For that we use the function **cpd2sparsetens**. The variables **idxs_sp** and **dims_sp** are the same of **idxs** and **dims**, we just repeated them to be concise with the format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.646602820780112e-15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the sparse format from the factors.\n",
    "data_approx, idxs_sp, dims_sp = tfx.cpd2sparsetens(factors, idxs, dims)\n",
    "\n",
    "# Compute the error.\n",
    "np.linalg.norm(data - data_approx)/np.linalg.norm(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **cpd2sparsetens** is flexible enough so we can even check for indexes outside **idxs**, that is, the values of the tensor we know that should be zero. This is a sanit check that can be helpful sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 41 indexes\n"
     ]
    }
   ],
   "source": [
    "# Generate a few indexes outside idxs.\n",
    "idxs_rnd = np.random.randint(0, 10, size=[1000, len(dims)])\n",
    "idxs_rnd = [idx for idx in idxs_rnd if idx not in idxs]\n",
    "print('Generated', len(idxs_rnd), 'indexes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  2.29473953e-67,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  3.86755905e-68,  0.00000000e+00,\n",
       "        9.25980378e-67,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -4.09807477e-71,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the values of the approximated tensor for these indexes. They should be close to zero. \n",
    "data_rnd_approx, idxs_sp, dims_sp = tfx.cpd2sparsetens(factors, idxs_rnd, dims)\n",
    "data_rnd_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a small example we can put everything in dense format and verify what is the actual error (this won't be possible for really large tensors). Regardless, the main point of the CPD is to approximate the nonzero entries, which is always accomplished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6682143314012025e-15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the coordinate (dense) format of the approximation.\n",
    "T_approx = tfx.cpd2tens(factors)\n",
    "\n",
    "# Generate the dense format from the sparse representation.\n",
    "T_dense = tfx.sparse2dense(data, idxs, dims)\n",
    "\n",
    "# Compute the error.\n",
    "np.linalg.norm(T_dense - T_approx)/np.linalg.norm(T_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a big reduction in memory cost when working with sparse representations. Below we show a graph with the maximum memory cost attained in the computation of the CPDs of sparse $n \\times n \\times n$ tensors (blue curve) vs. the cost to store these tensors in dense format. As we can see, the difference is substantial, Tensor Fox does avoid the intermediate memory explosion problem. For instance, the sparse approach requires $3648$ megabytes when $n = 30000$, whereas the dense approach requires approximately $205$ terabytes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sparse](sparse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big example\n",
    "\n",
    "The amazing memory reduction showed above is due to the fact that Tensor Fox needs $\\mathcal{0}(I_\\ell^2)$ flops of memory for each unfolding whereas the other classic methods needs $\\mathcal{O}(\\prod_{k \\neq \\ell} I_k)$ flops. Still, one can face tensor with dimensions at the order of billions or more, and in these cases even Tensor Fox will blow up the memory. To address this issue one can \"divide\" the dimensions in smaller pieces and then compute a CPD in a space with more dimensions, but lower ones. This approach is computationally feasible and can find good solutions. For more detail about this approach see (). \n",
    "\n",
    "In the example we generate a fourth order tensor $10^3 \\times 10^3 \\times 10^3 \\times 10^3$ with rank $R = 5$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor matrix sparsity level = 99.8 %\n",
      "Tensor sparsity level = 99.999999 %\n"
     ]
    }
   ],
   "source": [
    "# Dimensions of the tensor.\n",
    "dims = [10**3, 10**3, 10**3, 10**3]\n",
    "\n",
    "# Rank of the tensor.\n",
    "R = 5\n",
    "\n",
    "# Number of nonzero entries of each factor matrix. \n",
    "nnz = 10\n",
    "\n",
    "# Generate sparse tensor.\n",
    "factor_dims_prod = tfx.multiply_dims([dims[0], R])\n",
    "print('Factor matrix sparsity level =', round(100 * (factor_dims_prod - nnz)/factor_dims_prod, 2), '%')\n",
    "data, idxs, dims, factors = tfx.gen_rand_sparse_tensor(dims, R, nnz)\n",
    "tensor_dims_prod = tfx.multiply_dims(dims)\n",
    "print('Tensor sparsity level =', round(100 * (tensor_dims_prod - len(data))/tensor_dims_prod, 7), '%')\n",
    "\n",
    "# Define sparse tensor.\n",
    "T = [data, idxs, dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse tensor detected\n",
      "    nnz = 10000\n",
      "    Sparsity level = 99.999999 %\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    Compressing unfolding mode 1\n",
      "    Compressing unfolding mode 2\n",
      "    Compressing unfolding mode 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "->  mkl_sparse_d_spmmd returned 5 (SPARSE_STATUS_INTERNAL_ERROR).\n",
      "-> Using sparse-sparse dot from Tensor Fox.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Compressing unfolding mode 4\n",
      "    Computing sparse multilinear multiplication\n",
      "    Compression detected\n",
      "    Compressing from (1000, 1000, 1000, 1000) to (33, 33, 33, 33)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 35\n",
      "    Relative error = 6.908139757385737e-07\n",
      "    Accuracy =  99.99993 %\n"
     ]
    }
   ],
   "source": [
    "options.display = 1\n",
    "factors, output = tfx.cpd(T, R, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular example note that the mode 4 unfolding raised a warning. The program has three routines to perform sparse matrix multiplication, in order: the package *sparse_dot_mkl*, *Scipy* and Tensor Fox's routine itself. This order relies solely on speed. When the first one failed, the program used the sparse matrix multiplication from Scipy. The sparse multiplication from Tensor Fox usually will be called only when the tensor is so large that it can't be handled by none of the previous routines. With that regard Tensor Fox sparse multiplication can handle sparse tensors as long as the number on nonzero entries fits in memory.\n",
    "\n",
    "We finish with a few observations about the parameter $\\verb|mkl| \\_ \\verb|dot|$. By default the program uses a specific MKL sparse dot function to perform matrix-matrix multiplication. If the package *sparse_dot_mkl* is not installed, then the program uses the sparse dot product from Scipy. You can set $\\verb|mkl| \\_ \\verb|dot| = False$ to avoid the package *sparse_dot_mkl* completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the compressed tensor instead of the original one\n",
    "\n",
    "Remember lesson 8, where we computed the compression by ourselves and used it to compute the CPD of the original tensor. This approach is worth doing when we want to repeat the CPD computation several times without wasting time recomputing the compression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Compressing unfolding mode 1\n",
      "    Compressing unfolding mode 2\n",
      "    Compressing unfolding mode 3\n",
      "    Compressing unfolding mode 4\n",
      "    Computing sparse multilinear multiplication\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.090167469103268e-15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start creating the class of options.\n",
    "class options:    \n",
    "    # Define the error tolerance but do not use a value <= 0 to compute the compression directly like we are doing now.\n",
    "    tol_mlsvd = 1e-16  \n",
    "    display = 3\n",
    "    \n",
    "# We to compute the norm of the tensor in this case.\n",
    "Tsize = np.linalg.norm(data)\n",
    "    \n",
    "# Call the MLSVD function. Only S and U matters for us.\n",
    "S, U, T1, sigmas, error = tfx.mlsvd(T, Tsize, R, options) \n",
    "\n",
    "# Show the error of the MLSD. Remember that this is costly, usually one wants to use display < 3 to avoid this.\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    No compression and no truncation requested by user\n",
      "    Working with dimensions (33, 33, 33, 33)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 27\n",
      "    Relative error = 1.0100600817671315e-06\n",
      "    Accuracy =  99.9999 %\n"
     ]
    }
   ],
   "source": [
    "# Now we compute the CPD of S.\n",
    "options.tol_mlsvd = -1    # this options makes the program to skip the compression step, which is unnecessary now\n",
    "options.display = 1 \n",
    "factors, output = tfx.cpd(S, R, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have that T = (U1, U2, U3)*S = (U1, U2, U3)*(W1, W2, W3)*I = (U1*W1, U2*W2, U3*W3)*I, where W1, W2, W3 are the\n",
    "# factor matrices of S. Therefore U1*W1, U2*W2, U3*W3 are the factor matrices of T, se we redefine them.\n",
    "factors = [np.dot(u, w) for u, w in zip(U, factors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the final error we use again the function **cpd2sparsetens**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0082579808388269e-06"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the sparse format from the factors.\n",
    "data_approx, idxs_sp, dims_sp = tfx.cpd2sparsetens(factors, idxs, dims)\n",
    "\n",
    "# Compute the error.\n",
    "np.linalg.norm(data - data_approx)/np.linalg.norm(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cg_factor = 1 cg_tol = 1e-16 cg_maxiter = 10 rel_error = 4.478197273100179e-06\n",
      "cg_factor = 1 cg_tol = 1e-16 cg_maxiter = 50 rel_error = 0.12009074244212747\n",
      "cg_factor = 1 cg_tol = 1e-16 cg_maxiter = 100 rel_error = 2.2172198959001732e-06\n",
      "cg_factor = 1 cg_tol = 1e-32 cg_maxiter = 10 rel_error = 1.0280437091745793e-07\n",
      "cg_factor = 1 cg_tol = 1e-32 cg_maxiter = 50 rel_error = 4.241515281567776e-07\n",
      "cg_factor = 1 cg_tol = 1e-32 cg_maxiter = 100 rel_error = 2.1558262259107273e-07\n",
      "cg_factor = 2 cg_tol = 1e-16 cg_maxiter = 10 rel_error = 1.8568650678040798e-07\n",
      "cg_factor = 2 cg_tol = 1e-16 cg_maxiter = 50 rel_error = 2.7872753687256217e-07\n",
      "cg_factor = 2 cg_tol = 1e-16 cg_maxiter = 100 rel_error = 3.274859868406347e-07\n",
      "cg_factor = 2 cg_tol = 1e-32 cg_maxiter = 10 rel_error = 1.2428499203146418e-06\n",
      "cg_factor = 2 cg_tol = 1e-32 cg_maxiter = 50 rel_error = 3.928142799293109e-07\n",
      "cg_factor = 2 cg_tol = 1e-32 cg_maxiter = 100 rel_error = 5.439456871172553e-06\n",
      "cg_factor = 5 cg_tol = 1e-16 cg_maxiter = 10 rel_error = 3.257505401545529e-07\n",
      "cg_factor = 5 cg_tol = 1e-16 cg_maxiter = 50 rel_error = 4.006434106686048e-06\n",
      "cg_factor = 5 cg_tol = 1e-16 cg_maxiter = 100 rel_error = 1.0278891199405796e-07\n",
      "cg_factor = 5 cg_tol = 1e-32 cg_maxiter = 10 rel_error = 4.804147066761471e-06\n",
      "cg_factor = 5 cg_tol = 1e-32 cg_maxiter = 50 rel_error = 2.184186732608219e-07\n",
      "cg_factor = 5 cg_tol = 1e-32 cg_maxiter = 100 rel_error = 5.447364528191455e-07\n",
      "cg_factor = 10 cg_tol = 1e-16 cg_maxiter = 10 rel_error = 3.702254070050594e-07\n",
      "cg_factor = 10 cg_tol = 1e-16 cg_maxiter = 50 rel_error = 2.740060025289072e-07\n",
      "cg_factor = 10 cg_tol = 1e-16 cg_maxiter = 100 rel_error = 2.1591761118605416e-07\n",
      "cg_factor = 10 cg_tol = 1e-32 cg_maxiter = 10 rel_error = 9.858984586062945e-08\n",
      "cg_factor = 10 cg_tol = 1e-32 cg_maxiter = 50 rel_error = 6.568410954868115e-07\n",
      "cg_factor = 10 cg_tol = 1e-32 cg_maxiter = 100 rel_error = 3.892822204598354e-06\n",
      "Best model: cg_factor = 10 cg_tol = 1e-32 cg_maxiter = 10\n",
      "Final best rel_error = 9.858984586062945e-08\n"
     ]
    }
   ],
   "source": [
    "# Just like in lesson 8, we try several parameter customization in order to find  a better solution.\n",
    "best_error = np.inf\n",
    "\n",
    "for cg_factor in [1, 2, 5, 10]:\n",
    "    for cg_tol in [1e-16, 1e-32]:\n",
    "        for cg_maxiter in [10, 50, 100]:\n",
    "            options.tol_mlsvd = -1 \n",
    "            options.display = 0\n",
    "            options.cg_factor = cg_factor\n",
    "            options.cg_tol = cg_tol\n",
    "            options.cg_maxiter = cg_maxiter\n",
    "            factors, output = tfx.cpd(S, R, options)\n",
    "            print('cg_factor =', cg_factor, 'cg_tol =', cg_tol, 'cg_maxiter =', cg_maxiter, 'rel_error =', output.rel_error)\n",
    "            \n",
    "            if output.rel_error < best_error:\n",
    "                best_error = output.rel_error\n",
    "                best_factors = [np.dot(u, w) for u, w in zip(U, factors)]\n",
    "                best_output = copy(output)\n",
    "    \n",
    "output = best_output\n",
    "factors = best_factors\n",
    "print('Best model: cg_factor =', output.options.cg_factor, 'cg_tol =', output.options.cg_tol, 'cg_maxiter =', output.options.cg_maxiter)\n",
    "print('Final best rel_error =', output.rel_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
