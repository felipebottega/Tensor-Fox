{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse tensors\n",
    "\n",
    "A new functionality of Tensor Fox is the support for sparse tensors. Let $T \\in \\mathbb{R}^{I_1 \\times \\ldots \\times I_L}$ be a sparse tensor with *nnz* nonzero entries. This tensor is represented as a triple *[data, idxs, dims]*, where *data* is an array of size $nnz$ such that *data[i]* is the $i$-th nonzero entry of $T$, with corresponding index *idxs[i]*. It is necessary to pass *idxs* also as an array of shape $nnz \\times L$. Finally, we have that *dims* $= [I_1, \\ldots, I_L]$ are the dimwnsions of the tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import TensorFox as tfx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small example\n",
    "\n",
    "Below we start with a fourth order sparse tensor $10 \\times 10 \\times 10 \\times 10$ with rank $R = 5$ such that each factor matrix has 5 nonzero entries. We can generate sparse factor matrices with the function *gen_rand_sparse_tensor* with inputs **dims**, **R**, **nnz** as shown below. We remark that the sparsity of the tensor is not necessarily equal to the sparsity of its factor matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor matrix sparsity level = 80.0 %\n",
      "Tensor sparsity level = 87.4 %\n"
     ]
    }
   ],
   "source": [
    "# Dimensions of the tensor.\n",
    "dims = [10, 10, 10, 10]\n",
    "\n",
    "# Rank of the tensor.\n",
    "R = 5\n",
    "\n",
    "# Number of nonzero entries of each factor matrix. \n",
    "nnz = 10\n",
    "\n",
    "# Generate sparse tensor.\n",
    "factor_dims_prod = tfx.multiply_dims([dims[0], R])\n",
    "print('Factor matrix sparsity level =', round(100 * (factor_dims_prod - nnz)/factor_dims_prod, 2), '%')\n",
    "data, idxs, dims, factors = tfx.gen_rand_sparse_tensor(dims, R, nnz)\n",
    "tensor_dims_prod = tfx.multiply_dims(dims)\n",
    "print('Tensor sparsity level =', round(100 * (tensor_dims_prod - len(data))/tensor_dims_prod, 2), '%')\n",
    "\n",
    "# Define sparse tensor.\n",
    "T = [data, idxs, dims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remark that it is not possible to use $\\verb|tol| \\_ \\verb|mlsvd| = -1$ for sparse tensors. In the case the user sets $\\verb|display| = 3$ or $4$, the program computes only the error associated with the nonzero entries of $T$, otherwise we would face memory issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse tensor detected\n",
      "    nnz = 1260\n",
      "    Sparsity level = 87.4 %\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    Compressing unfolding mode 1\n",
      "    Compressing unfolding mode 2\n",
      "    Compressing unfolding mode 3\n",
      "    Compressing unfolding mode 4\n",
      "    Compression detected\n",
      "    Compressing from (10, 10, 10, 10) to (5, 5, 5, 5)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 85\n",
      "    Relative error = 1.2244622813077945e-14\n",
      "    Accuracy =  100.0 %\n"
     ]
    }
   ],
   "source": [
    "class options:\n",
    "    display = 1\n",
    "    \n",
    "factors, output = tfx.cpd(T, R, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative error above only take in account the nonzero entries of $T$ (the same is true for the compression error when displayed). The factor matrices of the decomposition may introduce small errors when approximating the zeros, and these small errors summed together does increase the actual error of the CPD. Thus the relative error above is a lower bound to the actual error, but usually close enough.\n",
    "\n",
    "We can convert the factor matrices to tensor sparse format just like the original input. For that we use the function **cpd2sparsetens**. The variables **idxs_sp** and **dims_sp** are the same of **idxs** and **dims**, we just repeated them to be concise with the format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2244622813077945e-14"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the sparse format from the factors.\n",
    "data_approx, idxs_sp, dims_sp = tfx.cpd2sparsetens(factors, idxs, dims)\n",
    "\n",
    "# Compute the error.\n",
    "np.linalg.norm(data - data_approx)/np.linalg.norm(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **cpd2sparsetens** is flexible enough so we can even check for indexes outside **idxs**, that is, the values of the tensor we know that should be zero. This is a sanit check that can be helpful sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 25 indexes\n"
     ]
    }
   ],
   "source": [
    "# Generate a few indexes outside idxs.\n",
    "idxs_rnd = np.random.randint(0, 10, size=[1000, len(dims)])\n",
    "idxs_rnd = [idx for idx in idxs_rnd if idx not in idxs]\n",
    "print('Generated', len(idxs_rnd), 'indexes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.31813972e-67,  0.00000000e+00,  0.00000000e+00,  2.67666419e-66,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  7.69648378e-66,\n",
       "        0.00000000e+00,  2.71622246e-67,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  1.11853999e-66,  8.28962224e-79,\n",
       "        0.00000000e+00])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the values of the approximated tensor for these indexes. They should be close to zero. \n",
    "data_rnd_approx, idxs_sp, dims_sp = tfx.cpd2sparsetens(factors, idxs_rnd, dims)\n",
    "data_rnd_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a small example we can put everything in dense format and verify what is the actual error (this won't be possible for really large tensors). Regardless, the main point of the CPD is to approximate the nonzero entries, which is always accomplished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.224164023490288e-14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the coordinate (dense) format of the approximation.\n",
    "T_approx = tfx.cpd2tens(factors)\n",
    "\n",
    "# Generate the dense format from the sparse representation.\n",
    "T_dense = tfx.sparse2dense(data, idxs, dims)\n",
    "\n",
    "# Compute the error.\n",
    "np.linalg.norm(T_dense - T_approx)/np.linalg.norm(T_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a big reduction in memory cost when working with sparse representations. Below we show a graph with the maximum memory cost attained in the computation of the CPDs of sparse $n \\times n \\times n$ tensors (blue curve) vs. the cost to store these tensors in dense format. As we can see, the difference is substantial, Tensor Fox does avoid the intermediate memory explosion problem. For instance, the sparse approach requires $3648$ megabytes when $n = 30000$, whereas the dense approach requires approximately $205$ terabytes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sparse](sparse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big example\n",
    "\n",
    "The amazing memory reduction showed above is due to the fact that Tensor Fox needs $\\mathcal{0}(I_\\ell^2)$ flops of memory for each unfolding whereas the other classic methods needs $\\mathcal{O}(\\prod_{k \\neq \\ell} I_k)$ flops. Still, one can face tensor with dimensions at the order of billions or more, and in these cases even Tensor Fox will blow up the memory. To address this issue one can \"divide\" the dimensions in smaller pieces and then compute a CPD in a space with more dimensions, but lower ones. This approach is computationally feasible and can find good solutions. For more detail about this approach see (). \n",
    "\n",
    "In the example we generate a fourth order tensor $10^3 \\times 10^3 \\times 10^3 \\times 10^3$ with rank $R = 5$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor matrix sparsity level = 99.8 %\n",
      "Tensor sparsity level = 99.999999 %\n"
     ]
    }
   ],
   "source": [
    "# Dimensions of the tensor.\n",
    "dims = [10**3, 10**3, 10**3, 10**3]\n",
    "\n",
    "# Rank of the tensor.\n",
    "R = 5\n",
    "\n",
    "# Number of nonzero entries of each factor matrix. \n",
    "nnz = 10\n",
    "\n",
    "# Generate sparse tensor.\n",
    "factor_dims_prod = tfx.multiply_dims([dims[0], R])\n",
    "print('Factor matrix sparsity level =', round(100 * (factor_dims_prod - nnz)/factor_dims_prod, 2), '%')\n",
    "data, idxs, dims, factors = tfx.gen_rand_sparse_tensor(dims, R, nnz)\n",
    "tensor_dims_prod = tfx.multiply_dims(dims)\n",
    "print('Tensor sparsity level =', round(100 * (tensor_dims_prod - len(data))/tensor_dims_prod, 7), '%')\n",
    "\n",
    "# Define sparse tensor.\n",
    "T = [data, idxs, dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse tensor detected\n",
      "    nnz = 10000\n",
      "    Sparsity level = 99.999999 %\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing MLSVD\n",
      "    Compressing unfolding mode 1\n",
      "    Compressing unfolding mode 2\n",
      "    Compressing unfolding mode 3\n",
      "    Compressing unfolding mode 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "->  mkl_sparse_d_spmmd returned 5 (SPARSE_STATUS_INTERNAL_ERROR). Using standard scipy dot.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Compression detected\n",
      "    Compressing from (1000, 1000, 1000, 1000) to (5, 5, 5, 5)\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Type of initialization: random\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Computing CPD\n",
      "===============================================================================================\n",
      "Final results\n",
      "    Number of steps = 64\n",
      "    Relative error = 4.6285761822170106e-14\n",
      "    Accuracy =  100.0 %\n"
     ]
    }
   ],
   "source": [
    "options.display = 1\n",
    "factors, output = tfx.cpd(T, R, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular example note that the mode 4 unfolding raised a warning. The program has three routines to perform sparse matrix multiplication, in order: the package *sparse_dot_mkl*, *Scipy* and Tensor Fox's routine itself. This order relies solely on speed. When the first one failed, the program used the sparse matrix multiplication from Scipy. The sparse multiplication from Tensor Fox usually will be called only when the tensor is so large that it can't be handled by none of the previous routines. With that regard Tensor Fox sparse multiplication can handle sparse tensors as long as the number on nonzero entries fits in memory.\n",
    "\n",
    "We finish with a few observations about the parameter $\\verb|mkl| \\_ \\verb|dot|$. By default the program uses a specific MKL sparse dot function to perform matrix-matrix multiplication. If the package *sparse_dot_mkl* is not installed, then the program uses the sparse dot product from Scipy. You can set $\\verb|mkl| \\_ \\verb|dot| = False$ to avoid the package *sparse_dot_mkl* completely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
